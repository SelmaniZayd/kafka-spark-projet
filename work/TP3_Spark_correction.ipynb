{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Chargement de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Lire de la donnée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Lecture brute\n",
    "\n",
    "Chargez le fichier ville_1.csv dans une variable nommée df.\n",
    "\n",
    "Vous pouvez afficher votre donnée en utilisant la méthode take() ou la methode collect() de l'objet pyspark DataFrame (attention appeler collect() sur un dataframe est déconseillé si vous avez du vrai big data).\n",
    "\n",
    "L'objet possède aussi un attribut appelé dtypes, appelez cet attribut pour obtenir la liste des colonnes et leur type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string'),\n",
       " ('_c5', 'string'),\n",
       " ('_c6', 'string'),\n",
       " ('_c7', 'string'),\n",
       " ('_c8', 'string'),\n",
       " ('_c9', 'string'),\n",
       " ('_c10', 'string'),\n",
       " ('_c11', 'string'),\n",
       " ('_c12', 'string')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./data/Villes/ville_1.csv\"\n",
    "# lecture d'un fichier de manière la plus brute\n",
    "df = spark.read.load(path, format=\"csv\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='id', _c1='vitesse_a_pied', _c2='vitesse_a_velo', _c3='home', _c4='travail', _c5='sportif', _c6='casseur', _c7='statut', _c8='salaire', _c9='sexe', _c10='age', _c11='sportivite', _c12='velo_perf_minimale'),\n",
       " Row(_c0='5251', _c1='0.02', _c2='0.05', _c3='(lon:26.60 lat:28.13)', _c4='(lon:21.08 lat:14.11)', _c5='False', _c6='False', _c7='reserviste', _c8='29800.610034665042', _c9='F', _c10='18', _c11='0.1', _c12='0.4'),\n",
       " Row(_c0='5252', _c1='0.14974625830876215', _c2='0.37436564577190534', _c3='(lon:0.26 lat:42.61)', _c4='(lon:36.35 lat:33.28)', _c5='False', _c6='False', _c7='professeur', _c8='23595.44383981423', _c9='F', _c10='28', _c11='0.7487312915438107', _c12='0.4'),\n",
       " Row(_c0='5253', _c1='0.6309711587089704', _c2='1.6825897565572543', _c3='(lon:3.34 lat:13.95)', _c4='(lon:24.75 lat:48.15)', _c5='False', _c6='False', _c7='technicien_de_surface', _c8='18530.14776280135', _c9='H', _c10='65', _c11='2.103237195696568', _c12='0.4'),\n",
       " Row(_c0='5254', _c1='0.04009596300649916', _c2='0.10692256801733109', _c3='(lon:19.54 lat:43.69)', _c4='(lon:38.57 lat:42.65)', _c5='False', _c6='False', _c7='technicien_de_surface', _c8='18997.60281005325', _c9='H', _c10='26', _c11='0.13365321002166386', _c12='0.4')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Lecture avec les entêtes\n",
    "\n",
    "Recharger le même fichier mais cette fois-ci utilisez l'option header pour rajouter les noms de colonnes à votre df.\n",
    "\n",
    "Appelez l'attribut dtypes et comparez la sortie avec celle de la lecture brute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('vitesse_a_pied', 'string'),\n",
       " ('vitesse_a_velo', 'string'),\n",
       " ('home', 'string'),\n",
       " ('travail', 'string'),\n",
       " ('sportif', 'string'),\n",
       " ('casseur', 'string'),\n",
       " ('statut', 'string'),\n",
       " ('salaire', 'string'),\n",
       " ('sexe', 'string'),\n",
       " ('age', 'string'),\n",
       " ('sportivite', 'string'),\n",
       " ('velo_perf_minimale', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l'option 'header' permet de rajouter les noms des colonnes \n",
    "df = spark.read.format('csv').options(header=True).load(path)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='5251', vitesse_a_pied='0.02', vitesse_a_velo='0.05', home='(lon:26.60 lat:28.13)', travail='(lon:21.08 lat:14.11)', sportif='False', casseur='False', statut='reserviste', salaire='29800.610034665042', sexe='F', age='18', sportivite='0.1', velo_perf_minimale='0.4'),\n",
       " Row(id='5252', vitesse_a_pied='0.14974625830876215', vitesse_a_velo='0.37436564577190534', home='(lon:0.26 lat:42.61)', travail='(lon:36.35 lat:33.28)', sportif='False', casseur='False', statut='professeur', salaire='23595.44383981423', sexe='F', age='28', sportivite='0.7487312915438107', velo_perf_minimale='0.4'),\n",
       " Row(id='5253', vitesse_a_pied='0.6309711587089704', vitesse_a_velo='1.6825897565572543', home='(lon:3.34 lat:13.95)', travail='(lon:24.75 lat:48.15)', sportif='False', casseur='False', statut='technicien_de_surface', salaire='18530.14776280135', sexe='H', age='65', sportivite='2.103237195696568', velo_perf_minimale='0.4'),\n",
       " Row(id='5254', vitesse_a_pied='0.04009596300649916', vitesse_a_velo='0.10692256801733109', home='(lon:19.54 lat:43.69)', travail='(lon:38.57 lat:42.65)', sportif='False', casseur='False', statut='technicien_de_surface', salaire='18997.60281005325', sexe='H', age='26', sportivite='0.13365321002166386', velo_perf_minimale='0.4'),\n",
       " Row(id='5255', vitesse_a_pied='0.02', vitesse_a_velo='0.05', home='(lon:28.51 lat:41.70)', travail='(lon:17.67 lat:25.16)', sportif='False', casseur='False', statut='éboueur', salaire='23618.479750220806', sexe='F', age='50', sportivite='0.1', velo_perf_minimale='0.4')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Lecture avec les types détectés automatiquement\n",
    "\n",
    "Recharger le fichier avec  l'option inferShema.\n",
    "\n",
    "L'option 'inferSchema' permet de transformer les colonnes en types plus précis : entier  / booléens / chaines de caractères... bien sûr spark trouve les types uniquement si le fichier d'origine permet de les trouver de manière simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('vitesse_a_pied', 'double'),\n",
       " ('vitesse_a_velo', 'double'),\n",
       " ('home', 'string'),\n",
       " ('travail', 'string'),\n",
       " ('sportif', 'boolean'),\n",
       " ('casseur', 'boolean'),\n",
       " ('statut', 'string'),\n",
       " ('salaire', 'double'),\n",
       " ('sexe', 'string'),\n",
       " ('age', 'int'),\n",
       " ('sportivite', 'double'),\n",
       " ('velo_perf_minimale', 'double')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l'option 'inferSchema' permet de transformer les colonnes en \n",
    "# types plus précis : entier  / booléens / chaines de caractères...\n",
    "# bien sûr spark trouve les types uniquement si le fichier d'origine\n",
    "# permet de les trouver de manière simple\n",
    "df = spark.read.format('csv').options(header=True, inferSchema=True).load(path)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=5251, vitesse_a_pied=0.02, vitesse_a_velo=0.05, home='(lon:26.60 lat:28.13)', travail='(lon:21.08 lat:14.11)', sportif=False, casseur=False, statut='reserviste', salaire=29800.610034665042, sexe='F', age=18, sportivite=0.1, velo_perf_minimale=0.4),\n",
       " Row(id=5252, vitesse_a_pied=0.14974625830876215, vitesse_a_velo=0.37436564577190534, home='(lon:0.26 lat:42.61)', travail='(lon:36.35 lat:33.28)', sportif=False, casseur=False, statut='professeur', salaire=23595.44383981423, sexe='F', age=28, sportivite=0.7487312915438107, velo_perf_minimale=0.4),\n",
       " Row(id=5253, vitesse_a_pied=0.6309711587089704, vitesse_a_velo=1.6825897565572543, home='(lon:3.34 lat:13.95)', travail='(lon:24.75 lat:48.15)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=18530.14776280135, sexe='H', age=65, sportivite=2.103237195696568, velo_perf_minimale=0.4),\n",
       " Row(id=5254, vitesse_a_pied=0.04009596300649916, vitesse_a_velo=0.10692256801733109, home='(lon:19.54 lat:43.69)', travail='(lon:38.57 lat:42.65)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=18997.60281005325, sexe='H', age=26, sportivite=0.13365321002166386, velo_perf_minimale=0.4),\n",
       " Row(id=5255, vitesse_a_pied=0.02, vitesse_a_velo=0.05, home='(lon:28.51 lat:41.70)', travail='(lon:17.67 lat:25.16)', sportif=False, casseur=False, statut='éboueur', salaire=23618.479750220806, sexe='F', age=50, sportivite=0.1, velo_perf_minimale=0.4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) L'attribut schema\n",
    "\n",
    "Il vous permet d'afficher le schéma de votre df, avec pour chaque colonne son nom, son type, et si elle accepte les valeurs nulles ou non. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,IntegerType,true),StructField(vitesse_a_pied,DoubleType,true),StructField(vitesse_a_velo,DoubleType,true),StructField(home,StringType,true),StructField(travail,StringType,true),StructField(sportif,BooleanType,true),StructField(casseur,BooleanType,true),StructField(statut,StringType,true),StructField(salaire,DoubleType,true),StructField(sexe,StringType,true),StructField(age,IntegerType,true),StructField(sportivite,DoubleType,true),StructField(velo_perf_minimale,DoubleType,true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous avez aussi la méthode printSchema() qui permet d'afficher le shéma du df de manière plus lisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- vitesse_a_pied: double (nullable = true)\n",
      " |-- vitesse_a_velo: double (nullable = true)\n",
      " |-- home: string (nullable = true)\n",
      " |-- travail: string (nullable = true)\n",
      " |-- sportif: boolean (nullable = true)\n",
      " |-- casseur: boolean (nullable = true)\n",
      " |-- statut: string (nullable = true)\n",
      " |-- salaire: double (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sportivite: double (nullable = true)\n",
      " |-- velo_perf_minimale: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Ecriture de la dataframe sur le disque\n",
    "\n",
    "Sauvegardez le df sous différents formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) choix du format : csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/jovyan/work/data/Villes/csv already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.save.\n: org.apache.spark.sql.AnalysisException: path file:/home/jovyan/work/data/Villes/csv already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f05276a25301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/Villes/csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/jovyan/work/data/Villes/csv already exists.;'"
     ]
    }
   ],
   "source": [
    "df.write.format(\"csv\").save(\"./data/Villes/csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) choix du format : parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").save(\"./data/Villes/parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) choix du format : json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.save(\"./data/Villes/ville\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Lecture de différents formats\n",
    "\n",
    "Vous pouvez choisir de lire le df sous un format ou un autre en utilisant l'argument format dans la fonction spark.read.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-a14e99cd-84de-49f8-9dd9-39745f6a4d40-c000.json  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "# le ! vous permet d'executer des commandes dans votre terminal depuis le notebook\n",
    "!ls ./data/Villes/ville/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.load(\"./data/Villes/ville/\", format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.load(\"./data/Villes/parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Calculer des résultats : les actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Nombre de lignes : count\n",
    "\n",
    "Chargez les fichiers csv contenus dans le dossiers ./data/Cyclistes/ dans un df nommé cyclistes, puis comptez les lignes du dataframe obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4868396"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyclistes = spark.read.load(\"./data/Cyclistes/\", format=\"csv\", header=True, inferSchema=\"True\")\n",
    "cyclistes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher le schéma de ce nouveau df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- sur_velo: boolean (nullable = true)\n",
      " |-- velo: string (nullable = true)\n",
      " |-- vitesse: double (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      " |-- destination_finale: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cyclistes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez 10 lignes du df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=246, timestamp=datetime.datetime(2018, 1, 1, 0, 1), sur_velo=False, velo='False', vitesse=0.02, position='(lon:22.62 lat:9.63)', destination_finale='False'),\n",
       " Row(id=246, timestamp=datetime.datetime(2018, 1, 1, 0, 2), sur_velo=False, velo='False', vitesse=0.02, position='(lon:22.62 lat:9.63)', destination_finale='False'),\n",
       " Row(id=246, timestamp=datetime.datetime(2018, 1, 1, 0, 3), sur_velo=False, velo='False', vitesse=0.02, position='(lon:22.62 lat:9.63)', destination_finale='False'),\n",
       " Row(id=246, timestamp=datetime.datetime(2018, 1, 1, 0, 4), sur_velo=False, velo='False', vitesse=0.02, position='(lon:22.62 lat:9.63)', destination_finale='False'),\n",
       " Row(id=246, timestamp=datetime.datetime(2018, 1, 1, 0, 5), sur_velo=False, velo='False', vitesse=0.02, position='(lon:22.62 lat:9.63)', destination_finale='False'),\n",
       " Row(id=246, timestamp=datetime.datetime(2018, 1, 1, 0, 6), sur_velo=False, velo='False', vitesse=0.02, position='(lon:22.62 lat:9.63)', destination_finale='False'),\n",
       " Row(id=246, timestamp=datetime.datetime(2018, 1, 1, 0, 7), sur_velo=False, velo='False', vitesse=0.02, position='(lon:22.62 lat:9.63)', destination_finale='False'),\n",
       " Row(id=246, timestamp=datetime.datetime(2018, 1, 1, 0, 8), sur_velo=False, velo='False', vitesse=0.02, position='(lon:22.62 lat:9.63)', destination_finale='False'),\n",
       " Row(id=246, timestamp=datetime.datetime(2018, 1, 1, 0, 9), sur_velo=False, velo='False', vitesse=0.02, position='(lon:22.62 lat:9.63)', destination_finale='False'),\n",
       " Row(id=246, timestamp=datetime.datetime(2018, 1, 1, 0, 10), sur_velo=False, velo='False', vitesse=0.02, position='(lon:22.62 lat:9.63)', destination_finale='False')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyclistes.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Moyenne : agg + colonne + mean\n",
    "\n",
    "A l'aide de la méthode agg(), calculez la moyenne sur la colonne vitesse.\n",
    "\n",
    "Vous pouvez récuperer le résultat avec la méthode collect()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(vitesse)|\n",
      "+------------------+\n",
      "|0.5635749865179128|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cd cyclistes.agg({\"vitesse\" : \"mean\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Quantile approximatifs pour gagner du temps de calcul\n",
    "\n",
    "En statistiques et en théorie des probabilités, les quantiles sont les valeurs qui divisent un jeu de données en intervalles contenant le même nombre de données. Il y a donc un quantile de moins que le nombre de groupes créés. Ainsi les quartiles sont les trois quantiles qui divisent un ensemble de données en quatre groupes de taille égale.\n",
    "\n",
    "La méthode approxQuantile permet de laisser une tolérance a l'erreur ce qui réduit le temps de calul sur d'énormes jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_quantile(df, erreur_acceptee):\n",
    "    debut            = time.time()\n",
    "    colonne          = \"vitesse\"\n",
    "    quantiles_voulus = [0.25, 0.50, 0.75]\n",
    "    resultat         =  df.approxQuantile(colonne, quantiles_voulus , erreur_acceptee )\n",
    "    fin              = time.time()\n",
    "    delais           = fin -debut\n",
    "    print (\"delais =%.2f sec, quantiles = %s\"%(delais, resultat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcul_quantile(cyclistes, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcul_quantile(cyclistes, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcul_quantile(cyclistes, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload de la dataframe villes\n",
    "\n",
    "Chargez le fichier villes dans un df nommé villes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "villes =spark.read.load(\"./data/Villes/\", format=\"csv\", header=True, inferSchema=\"True\")\n",
    "villes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=5251, vitesse_a_pied=0.02, vitesse_a_velo=0.05, home='(lon:26.60 lat:28.13)', travail='(lon:21.08 lat:14.11)', sportif=False, casseur=False, statut='reserviste', salaire=29800.610034665042, sexe='F', age=18, sportivite=0.1, velo_perf_minimale=0.4),\n",
       " Row(id=5252, vitesse_a_pied=0.14974625830876215, vitesse_a_velo=0.37436564577190534, home='(lon:0.26 lat:42.61)', travail='(lon:36.35 lat:33.28)', sportif=False, casseur=False, statut='professeur', salaire=23595.44383981423, sexe='F', age=28, sportivite=0.7487312915438107, velo_perf_minimale=0.4),\n",
       " Row(id=5253, vitesse_a_pied=0.6309711587089704, vitesse_a_velo=1.6825897565572543, home='(lon:3.34 lat:13.95)', travail='(lon:24.75 lat:48.15)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=18530.14776280135, sexe='H', age=65, sportivite=2.103237195696568, velo_perf_minimale=0.4),\n",
       " Row(id=5254, vitesse_a_pied=0.04009596300649916, vitesse_a_velo=0.10692256801733109, home='(lon:19.54 lat:43.69)', travail='(lon:38.57 lat:42.65)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=18997.60281005325, sexe='H', age=26, sportivite=0.13365321002166386, velo_perf_minimale=0.4),\n",
       " Row(id=5255, vitesse_a_pied=0.02, vitesse_a_velo=0.05, home='(lon:28.51 lat:41.70)', travail='(lon:17.67 lat:25.16)', sportif=False, casseur=False, statut='éboueur', salaire=23618.479750220806, sexe='F', age=50, sportivite=0.1, velo_perf_minimale=0.4),\n",
       " Row(id=5256, vitesse_a_pied=0.8655449921165502, vitesse_a_velo=2.308119978977467, home='(lon:44.85 lat:45.65)', travail='(lon:9.18 lat:11.05)', sportif=False, casseur=False, statut='employe', salaire=19082.30894283764, sexe='H', age=57, sportivite=2.885149973721834, velo_perf_minimale=0.4),\n",
       " Row(id=5257, vitesse_a_pied=0.5879992290928728, vitesse_a_velo=1.4699980727321822, home='(lon:42.06 lat:43.69)', travail='(lon:0.51 lat:24.89)', sportif=False, casseur=False, statut='reserviste', salaire=21782.945135729053, sexe='F', age=75, sportivite=2.9399961454643644, velo_perf_minimale=0.4),\n",
       " Row(id=5258, vitesse_a_pied=0.8306610123216782, vitesse_a_velo=2.215096032857809, home='(lon:24.87 lat:16.06)', travail='(lon:36.80 lat:48.41)', sportif=False, casseur=False, statut='cadre', salaire=41451.270468058414, sexe='H', age=74, sportivite=2.768870041072261, velo_perf_minimale=0.4),\n",
       " Row(id=5259, vitesse_a_pied=0.12542885835859416, vitesse_a_velo=0.3344769556229178, home='(lon:9.07 lat:17.28)', travail='(lon:4.80 lat:9.81)', sportif=False, casseur=False, statut='technicien_de_surface', salaire=22025.17074872747, sexe='H', age=57, sportivite=0.4180961945286472, velo_perf_minimale=0.4),\n",
       " Row(id=5260, vitesse_a_pied=0.030000000000000006, vitesse_a_velo=0.08, home='(lon:37.12 lat:43.64)', travail='(lon:24.72 lat:13.53)', sportif=False, casseur=False, statut='reserviste', salaire=38211.06301426453, sexe='H', age=24, sportivite=0.1, velo_perf_minimale=0.4)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4) corrélation\n",
    "\n",
    "En probabilités et en statistique, la corrélation entre plusieurs variables aléatoires ou statistiques est une notion de liaison qui contredit leur indépendance.\n",
    "\n",
    "Calculez la corrélation entre les colonnes age et vitesse_a_velo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8736306718547594"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes.corr(\"vitesse_a_velo\", \"sportivite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5) covariance\n",
    "\n",
    "La covariance entre deux variables aléatoires est un nombre permettant de quantifier leurs écarts conjoints par rapport à leurs espérances respectives. Elle s’utilise également pour deux séries de données numériques (écarts par rapport aux moyennes).\n",
    "La covariance est une extension de la notion de variance. La corrélation est une forme normalisée de la covariance.\n",
    "\n",
    "Calculez la covariance entre les colonnes age et vitesse_a_velo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes.cov(\"age\", \"vitesse_a_velo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6) sample\n",
    "\n",
    "La méthode sample() permet de tirer aléatoirement une fraction du dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes_1_pct = villes.sample(False, 0.01)\n",
    "villes_1_pct.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes_1_pct.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes.exceptAll(villes_1_pct).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7) filter \n",
    "\n",
    "La méthode filter() permet le df selon certaines valeurs dans les colonnes.\n",
    "\n",
    "Utilisez cette méthode pour récuperer seulement les lignes avec le sexe féminin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villesF = villes.filter(villes[\"sexe\"]==\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villesF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "villesF.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peux aussi filtrer le df avec la méthode where(). Filtrez le df de la même façon que precedemment en utilisant cette méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes.where(villes.sexe==\"F\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Transformer la données : les transformations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations : demandent à être suivi par un collect ou une action (count par exemple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Obtenir des statistiques sur les colonnes numériques\n",
    "\n",
    "La méthode describe() permet de calculer les statistiques récapitulatives d'une ou plusieurs colonnes numériques dans un df. Si le nom des colonnes n'est pas spécifié, la méthode calculera des statistiques récapitulatives pour toutes les colonnes numériques présentes dans le df.\n",
    "\n",
    "Afficher les statistiques de la colonne age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "villes.describe([\"age\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes.describe([\"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) groupby\n",
    "\n",
    "La méthode groupBy() suivie de la methode agg() permet de grouper le df selon les catgories d'une ou plusieurs colonnes pour faire des calculs sur ces catégories.\n",
    "\n",
    "Calculez la moyenne de la colonnes sportivité selon le sexe des personnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes.groupBy(\"sexe\").agg({\"sportivite\" : \"mean\"}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculez la moyenne de la colonne age et la valeur max de la colonne sportivité par sexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes.groupBy(\"sexe\").agg({\"sportivite\" : \"max\", \"age\":\"mean\"}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculez la moyenne des colonnes vitesse_a_pied et vitesse_a_velo par sexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "villes.groupBy(\"sexe\").agg({\"vitesse_a_pied\" : \"mean\", \"vitesse_a_velo\":\"mean\"}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) summary\n",
    "\n",
    "La méthode summary() permet des faire des calculs statistiques de base sur toutes les colonnes du df.\n",
    "\n",
    "Appliquez un count et un max sur toutes les colonnes du df et afficher les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes.summary(\"count\", \"max\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4) Union de dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajouter les colonnes les unes à côté des autres : join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "villes.join(villes, on=\"id\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajouter les lignes les unes sous les autres : union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "villes.unionByName(villes).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6) Concaténation de colonne : F.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ici reprendre le df cyclistes.\n",
    "\n",
    "Utiliser les méthodes withColumn() et F.concat() pour ajouter une colonne au df qui contiendra la concatenation des valeurs des colonnes id et sur_velo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"./data/Cyclistes/*.csv\" \n",
    "cyclistes = spark.read.format(\"csv\").option(\"header\", \"true\").load(path, inferSchema=True)\n",
    "cyclistes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cyclistes.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclistes.withColumn(\"id_sur_velo\", F.concat(cyclistes.id, cyclistes.sur_velo)).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Fonctions udf \n",
    "Il est possible d'enregistrer des fonctions python que l'on écrit nous même pour les appliquer sur une colonne d'une dataframe, c'est ce qu'on appelle les udf, pour User Defined Functions.\n",
    "\n",
    "Voici une fonction qui prend en argument une colonne et calcule le carré des valeurs de cette colonne.\n",
    "Appliquez cette fonction sur la colonne salaire de votre df. Affichez le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType = FloatType())\n",
    "def cube(colonne):\n",
    "    return colonne*colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "villes.select(cube(\"salaire\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6)\tEtude de cas : analyse des fichiers de logs des cyclistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1)  Charger la donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/Cyclistes/*.csv\" \n",
    "cyclistes = spark.read.format(\"csv\").option(\"header\", \"true\").load(path, inferSchema=True)\n",
    "cyclistes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cyclistes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclistes.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2) vérifier le nombre de cycles\n",
    "\n",
    "Comptez le nombre d'id uniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = cyclistes.select(\"id\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3) transformer les timestamp en date\n",
    "\n",
    "Voici une fonction qui permert de récuperer la date sous forme de chaîne de caractère dans la colonne timestamps pour la transformer en date exploitable en tant que telle.\n",
    "\n",
    "Créez une nouvelle colonne dans votre df stockant le résultat de cette fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "@udf(returnType = TimestampType())\n",
    "def transform_timestamp_in_date(timestamp):\n",
    "    from datetime import datetime\n",
    "    return datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclistes = cyclistes.withColumn(\"date\", transform_timestamp_in_date(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cyclistes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclistes.select(\"date\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclistes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4) Durée des trajets par id.\n",
    "\n",
    "A partir d'ici, il s'agit de traiter votre donnée pour récupérer la durée de chaque trajet effectué par chaque id.\n",
    "\n",
    "1) trouvez les dates min/max par état de sur_velo, puis par id ET par état de sur_velo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cyclistes.groupBy([\"sur_velo\"]).agg(F.min(cyclistes.date), F.max(cyclistes.date))\n",
    "_.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cyclistes.sort([\"id\", \"date\"]).groupBy([\"id\", \"sur_velo\"])\\\n",
    "                      .agg(F.min(cyclistes.date), F.max(cyclistes.date)).sort(\"id\")\n",
    "_.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Le résultat n'est pas trés pertinent, il faudrait plutôt le début et la fin de chaque trajet par id. Pour cela, il faudrait détecter les changements d'états \"sur_vélo\".\n",
    "Utilisez la classe Window() et la fonction F.lag() pour créer une nouvelle colonne que vous appellerez changement, contenant un 0 si l'état précedent de sur_velo est le même et un 1 si l'état vient de changer (fonction changement() ci-dessous) pour chaque id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType = IntegerType())\n",
    "def changement(etat_actuel, etat_precedent):\n",
    "    \"\"\"\n",
    "    Détecte si les deux états sont différent.\n",
    "    \n",
    "    Parametres :\n",
    "        etat_actuel : valeur sur la ligne courante\n",
    "                      renvoyée par F.lag (0)\n",
    "        etat_precedent : valeur sur la ligne précédente\n",
    "                      renvoyée par F.lag(1)\n",
    "    Return: 0 s'ils sont égaux, 1 s'il y a une différence\n",
    "    \"\"\"\n",
    "    if etat_precedent == None:\n",
    "        return 0\n",
    "    if etat_precedent == etat_actuel:\n",
    "        return 0\n",
    "    if etat_actuel != etat_precedent:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "w = Window.orderBy([\"id\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclistes = cyclistes.withColumn(\"changement\", changement( F.lag(\"sur_velo\", 0).over(w), F.lag(\"sur_velo\", 1).over(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclistes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cyclistes.select(\"id\", \"timestamp\", \"velo\", \"changement\").sort(\"id\", \"timestamp\").take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Grâce à la fonction window appliquez la fonction somme() sur la colonne changement pour numeroter les trajets pour chaque id et stocker les résulats dans une nouvelle colonne appelée numero_de_trajet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType = IntegerType())\n",
    "def somme(indice_actuel, indice_precedent):\n",
    "    if indice_precedent == None:\n",
    "        return 0\n",
    "    return indice_actuel + indice_precedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowval = Window.orderBy([\"id\", \"date\"])\n",
    "windowval = windowval.partitionBy(\"id\")\n",
    "windowval = windowval.rangeBetween(Window.unboundedPreceding, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclistes = cyclistes.withColumn(\"numero_de_trajet\", F.sum(\"changement\").over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclistes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclistes.select(\"id\", \"timestamp\", \"changement\", \"numero_de_trajet\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Il suffit maintenant de repêter la première étape, c'est a dire récupérer la début et la fin de chaque trajet pour chaque id. Puis calculer la durée des trajets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    debut_fin_trajets = cyclistes.groupBy([\"id\", \"numero_de_trajet\"])\\\n",
    "                                        .agg(   F.min(cyclistes.date) , \n",
    "                                                F.max(cyclistes.date) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajets = cyclistes\n",
    "debut_fin_trajets = trajets.groupBy([\"id\", \"numero_de_trajet\"]).agg(F.min(cyclistes.date),F.max(cyclistes.date))\n",
    "debut_fin_trajets.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajets = trajets.groupBy([\"id\", \"numero_de_trajet\"])\n",
    "trajets = trajets.agg(F.unix_timestamp(F.max(cyclistes.date)) - F.unix_timestamp(F.min(cyclistes.date)))\n",
    "trajets = trajets.sort(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trajets = trajets.withColumnRenamed(\"(unix_timestamp(max(date), yyyy-MM-dd HH:mm:ss) - unix_timestamp(min(date), yyyy-MM-dd HH:mm:ss))\", \"duree\")\n",
    "trajets.take(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) datavisualisation\n",
    "\n",
    "Convertissez votre dataframe pyspark en dataframe pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = trajets.toPandas()\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de la librairie seaborn, réalisez un graphique en barre montrant la durée de tout les trajets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.sort_values([\"id\", \"numero_de_trajet\"]).plot(kind=\"bar\", x=\"numero_de_trajet\", y=\"duree\", title=\"Durée des trajets \")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire le même graphique mais cette fois-ci, faire en sorte qu'on puisse choisir un id et afficher seulement les trajets de cet id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_cycliste = [40]\n",
    "filtre = df_pandas.id.isin(numero_cycliste)\n",
    "df_pandas[filtre].plot(kind=\"bar\", x=\"numero_de_trajet\", y=\"duree\", title=\"Durée des trajets pour le cycliste %s\" %numero_cycliste)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegardez votre dataset trajets au format csv dans le dossier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trajets.write.format(\"csv\").save(\"./data/trajets1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux ce6ae5a6fcfb 5.4.39-linuxkit #1 SMP Fri May 8 23:03:06 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n"
     ]
    }
   ],
   "source": [
    "!uname -a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
